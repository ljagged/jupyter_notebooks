{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljagged/jupyter_notebooks/blob/master/Qwen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ AI Development Environment Setup with Ollama & Tailscale\n",
        "\n",
        "This notebook configures a complete, secure development environment in Google Colab. It sets up:\n",
        "\n",
        "1.  **Ollama Server**: Runs large language models like Qwen3.\n",
        "2.  **Local Models**: Loads models from your Google Drive for persistence and speed.\n",
        "3.  **Secure Networking**: Uses **Tailscale** to create a private, secure connection between Colab and your local machine. This is more secure and stable than using ngrok.\n",
        "\n",
        "### Instructions\n",
        "1.  Set your `TAILSCALE_AUTH_KEY` in the first code cell.\n",
        "2.  Run the cells in order by selecting `Runtime > Run all`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZu0KJhybkGj"
      },
      "source": [
        "import os\n",
        "\n",
        "# --- üîë CONFIGURATION --- #\n",
        "# 1. Get a Tailscale auth key from your admin console:\n",
        "#    https://login.tailscale.com/admin/settings/keys\n",
        "#    Choose an 'Ephemeral' and 'Reusable' key for best results with Colab.\n",
        "TAILSCALE_AUTH_KEY = \"YOUR_TAILSCALE_AUTH_KEY_HERE\" # üö® PASTE YOUR KEY HERE\n",
        "\n",
        "# --- Mount Google Drive & Setup Paths ---\n",
        "print(\"üîÑ Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "WORKSPACE_PATH = os.path.join(DRIVE_ROOT, \"ZedWorkspace\")\n",
        "MODEL_PATH = os.path.join(DRIVE_ROOT, \"ollamaModels\") # Using your existing folder name\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "print(f\"‚úÖ Google Drive mounted. Model path set to: {MODEL_PATH}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Install Dependencies (Ollama & Tailscale)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxEdWJ5tbzMT"
      },
      "source": [
        "%%bash\n",
        "echo \"üîÑ Installing Ollama...\"\n",
        "# Install Ollama\n",
        "curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1\n",
        "echo \"‚úÖ Ollama installed.\"\n",
        "\n",
        "echo \"üîÑ Installing Tailscale...\"\n",
        "# Install Tailscale\n",
        "curl -fsSL https://tailscale.com/install.sh | sh > /dev/null 2>&1\n",
        "echo \"‚úÖ Tailscale installed.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Connect to Secure Network (Tailscale)\n",
        "\n",
        "This cell connects your Colab instance to your private Tailscale network, making it securely accessible from your local machine without exposing it to the internet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tailscale_connect_cell"
      },
      "source": [
        "import subprocess\n",
        "import re\n",
        "import time\n",
        "\n",
        "if not TAILSCALE_AUTH_KEY or TAILSCALE_AUTH_KEY == \"YOUR_TAILSCALE_AUTH_KEY_HERE\":\n",
        "    print(\"üö® ERROR: Tailscale auth key is not set. Please set it in the first cell.\")\n",
        "else:\n",
        "    print(\"üöÄ Starting Tailscale...\")\n",
        "    # Start tailscale and authenticate\n",
        "    tailscale_command = f'tailscale up --authkey=\"{TAILSCALE_AUTH_KEY}\" --hostname=\"colab-ollama-runner\" --accept-routes'\n",
        "    subprocess.run(tailscale_command, shell=True, check=True, capture_output=True)\n",
        "    print(\"‚úÖ Tailscale is running.\")\n",
        "    \n",
        "    # It can take a few seconds for the IP to be assigned\n",
        "    time.sleep(5)\n",
        "    \n",
        "    # Get the Tailscale IP address\n",
        "    ip_result = subprocess.run(['tailscale', 'ip', '-4'], capture_output=True, text=True)\n",
        "    tailscale_ip = ip_result.stdout.strip()\n",
        "\n",
        "    if re.match(r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", tailscale_ip):\n",
        "        print(f\"üîë Your private Ollama IP is: {tailscale_ip}\")\n",
        "        # This variable will be used later to generate the Zed config\n",
        "        OLLAMA_PRIVATE_URL = f\"http://{tailscale_ip}:11434\"\n",
        "    else:\n",
        "        print(\"‚ùå Could not retrieve Tailscale IP. Please check your Tailscale admin console.\")\n",
        "        OLLAMA_PRIVATE_URL = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Start Ollama and Load Local Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyUe_1UUbAGM"
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"üîÑ Preparing to start Ollama server...\")\n",
        "\n",
        "# Kill any previous Ollama processes to ensure a clean start\n",
        "try:\n",
        "    subprocess.run([\"pkill\", \"-f\", \"ollama\"], check=False)\n",
        "    print(\"Ensured previous Ollama processes are stopped.\")\n",
        "    time.sleep(2)\n",
        "except Exception as e:\n",
        "    print(f\"Could not pkill ollama, continuing: {e}\")\n",
        "\n",
        "# Set up environment for the Ollama subprocess\n",
        "ollama_env = os.environ.copy()\n",
        "ollama_env['OLLAMA_HOST'] = \"0.0.0.0\"  # Listen on all interfaces\n",
        "ollama_env['OLLAMA_ORIGINS'] = \"*\"      # Allow all origins (safe within a private Tailscale network)\n",
        "\n",
        "print(f\"Starting Ollama server, listening on {ollama_env['OLLAMA_HOST']}...\")\n",
        "\n",
        "# Start Ollama in the background\n",
        "ollama_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    env=ollama_env,\n",
        "    preexec_fn=os.setsid  # Detach from this notebook's session\n",
        ")\n",
        "\n",
        "print(f\"Ollama server process started with PID: {ollama_process.pid}\")\n",
        "print(\"Waiting for server to initialize...\")\n",
        "time.sleep(15)\n",
        "\n",
        "# --- Create Models from Local Filesystem ---\n",
        "MODELS_TO_CREATE = {\n",
        "    \"nomic-embed-text-local\": \"nomic-embed-text.mod\",\n",
        "    \"qwen3-8b-local\": \"qwen3_8b.mod\",\n",
        "    \"qwen3-30b-local\": \"qwen3_30b-a3b.mod\"\n",
        "}\n",
        "print(\"\\nüîÑ Creating models from local Modelfiles...\")\n",
        "for model_name, modelfile_name in MODELS_TO_CREATE.items():\n",
        "    modelfile_path = os.path.join(MODEL_PATH, modelfile_name)\n",
        "    print(f\"Attempting to create '{model_name}' from '{modelfile_path}'...\")\n",
        "\n",
        "    if not os.path.exists(modelfile_path):\n",
        "        print(f\"   ‚ùå Error: Modelfile not found at {modelfile_path}. Skipping.\")\n",
        "        continue\n",
        "    try:\n",
        "        create_command = [\"ollama\", \"create\", model_name, \"-f\", modelfile_path]\n",
        "        result = subprocess.run(\n",
        "            create_command,\n",
        "            capture_output=True, text=True, check=True, timeout=900  # 15-minute timeout for large models\n",
        "        )\n",
        "        print(f\"   ‚úÖ Successfully created model '{model_name}'.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"   ‚ùå Failed to create model '{model_name}':\\n      {e.stderr.strip()}\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(f\"   ‚ùå Error: Command timed out while creating '{model_name}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå An unexpected error occurred: {e}\")\n",
        "\n",
        "# --- Final Verification --- #\n",
        "print(\"\\nüìä Verifying available models with 'ollama list'...\")\n",
        "try:\n",
        "    result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True, check=True, timeout=20)\n",
        "    print(\"‚úÖ Ollama is running with the following models:\")\n",
        "    print(result.stdout)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to list Ollama models: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Generate Zed Configuration\n",
        "\n",
        "This cell generates the `settings.json` content for Zed, using the private Tailscale IP for the connection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generate_zed_config_cell"
      },
      "source": [
        "import json\n",
        "\n",
        "if 'OLLAMA_PRIVATE_URL' in locals() and OLLAMA_PRIVATE_URL:\n",
        "    print(f\"Generating Zed configuration using private URL: {OLLAMA_PRIVATE_URL}\")\n",
        "\n",
        "    zed_settings = {\n",
        "        \"features\": {\n",
        "            \"edit_prediction_provider\": \"zed\"\n",
        "        },\n",
        "        \"language_models\": {\n",
        "            \"ollama\": {\n",
        "                \"api_url\": OLLAMA_PRIVATE_URL\n",
        "            }\n",
        "        },\n",
        "        \"agent\": {\n",
        "            \"version\": \"2\",\n",
        "            \"default_model\": {\n",
        "                \"provider\": \"ollama\",\n",
        "                \"model\": \"qwen3-8b-local\"\n",
        "            },\n",
        "            \"inline_assistant_model\": {\n",
        "                \"provider\": \"ollama\",\n",
        "                \"model\": \"qwen3-8b-local\"\n",
        "            },\n",
        "            \"thread_summary_model\": {\n",
        "                \"provider\": \"ollama\",\n",
        "                \"model\": \"qwen3-8b-local\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Display the configuration\n",
        "    print(\"\\nüìã Copy the following JSON into your Zed 'settings.json' file:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(json.dumps(zed_settings, indent=2))\n",
        "    print(\"-\" * 60)\n",
        "else:\n",
        "    print(\"‚ùå Cannot generate Zed config: Tailscale IP was not retrieved successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Final Instructions & Keep-Alive\n",
        "\n",
        "Your environment is now running. This final cell will keep the Colab instance active."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PN4fawcdAF4"
      },
      "source": [
        "import time\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üéâ SETUP COMPLETE! Your secure AI development environment is live.\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìã NEXT STEPS ON YOUR LOCAL COMPUTER:\")\n",
        "print(\"1. **Install Tailscale**: Download and install it from https://tailscale.com/download\")\n",
        "print(\"2. **Log In to Tailscale**: Use the same account you used to generate the auth key.\")\n",
        "print(\"3.  **Check Connection**: Run `tailscale status` in your local terminal. You should see the `colab-ollama-runner` machine listed.\")\n",
        "4.  **Configure Zed**: Copy the JSON configuration from the cell above into your Zed `settings.json` file.\n",
        "5.  **Restart Zed**: Ensure Zed picks up the new settings.\n",
        "\n",
        "if 'tailscale_ip' in locals() and tailscale_ip:\n",
        "    print(f\"\\nYour private Ollama server is now accessible *only* to you at: http://{tailscale_ip}:11434\")\n",
        "else:\n",
        "    print(\"\\nCould not determine private Ollama IP. Please check the Tailscale setup cell.\")\n",
        "\n",
        "print(\"\\nThis cell will now run indefinitely to keep the Colab instance and Tailscale connection active.\")\n",
        "print(\"Press the 'Stop' button on this cell when you are finished to shut down the environment.\")\n",
        "\n",
        "# Keep-alive loop\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(300) # Sleep for 5 minutes\n",
        "        print(f\"[{time.strftime('%H:%M:%S')}] Keep-alive check. All services running.\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüõë Shutdown requested. All processes will terminate.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
