{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljagged/jupyter_notebooks/blob/master/qwen_tailscale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_Lol_zSQ4Fw"
      },
      "source": [
        "## üöÄ AI Development Environment Setup with Ollama & Tailscale\n",
        "\n",
        "This notebook configures a complete, secure development environment in Google Colab. It sets up:\n",
        "\n",
        "1.  **Ollama Server**: Runs large language models like Qwen3.\n",
        "2.  **Local Models**: Loads models from your Google Drive for persistence and speed.\n",
        "3.  **Secure Networking**: Uses **Tailscale** to create a private, secure connection between Colab and your local machine. This is more secure and stable than using ngrok.\n",
        "\n",
        "### Instructions\n",
        "1.  Set your `TAILSCALE_AUTH_KEY` in the first code cell.\n",
        "2.  Run the cells in order by selecting `Runtime > Run all`."
      ],
      "id": "c_Lol_zSQ4Fw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZu0KJhybkGj",
        "outputId": "1992fe17-ab10-4730-806d-46e5e7f1b5e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# --- üîë CONFIGURATION --- #\n",
        "# 1. Get a Tailscale auth key from your admin console:\n",
        "#    https://login.tailscale.com/admin/settings/keys\n",
        "#    Choose an 'Ephemeral' and 'Reusable' key for best results with Colab.\n",
        "TAILSCALE_AUTH_KEY = \"tskey-auth-kUtkhfQNMS11CNTRL-F8uEXRohNLVyAirK9JCaLVejEj5doARt\" # üö® PASTE YOUR KEY HERE\n",
        "\n",
        "# --- Mount Google Drive & Setup Paths ---\n",
        "print(\"üîÑ Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "WORKSPACE_PATH = os.path.join(DRIVE_ROOT, \"ZedWorkspace\")\n",
        "MODEL_PATH = os.path.join(DRIVE_ROOT, \"ollamaModels\") # Using your existing folder name\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "print(f\"‚úÖ Google Drive mounted. Model path set to: {MODEL_PATH}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted. Model path set to: /content/drive/MyDrive/ollamaModels\n"
          ]
        }
      ],
      "id": "GZu0KJhybkGj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FEfIp42Q4Fy"
      },
      "source": [
        "### Step 2: Install Dependencies (Ollama & Tailscale)"
      ],
      "id": "9FEfIp42Q4Fy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxEdWJ5tbzMT",
        "outputId": "dd0133ae-a1fc-48fe-cd2c-2dee0879b090",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%bash\n",
        "echo \"üîÑ Installing Ollama...\"\n",
        "# Install Ollama\n",
        "curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1\n",
        "echo \"‚úÖ Ollama installed.\"\n",
        "\n",
        "echo \"üîÑ Installing Tailscale...\"\n",
        "# Install Tailscale\n",
        "curl -fsSL https://tailscale.com/install.sh | sh > /dev/null 2>&1\n",
        "echo \"‚úÖ Tailscale installed.\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Installing Ollama...\n",
            "‚úÖ Ollama installed.\n",
            "üîÑ Installing Tailscale...\n",
            "‚úÖ Tailscale installed.\n"
          ]
        }
      ],
      "id": "HxEdWJ5tbzMT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlRimYJOQ4Fy"
      },
      "source": [
        "### Step 3: Connect to Secure Network (Tailscale)\n",
        "\n",
        "This cell connects your Colab instance to your private Tailscale network, making it securely accessible from your local machine without exposing it to the internet."
      ],
      "id": "QlRimYJOQ4Fy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tailscale_connect_cell",
        "outputId": "c2486eb1-dfc9-418e-c84c-b534fa4de8f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Step 3: Connect to Secure Network (Tailscale)\n",
        "import subprocess\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "\n",
        "if not TAILSCALE_AUTH_KEY or TAILSCALE_AUTH_KEY == \"YOUR_TAILSCALE_AUTH_KEY_HERE\":\n",
        "    print(\"üö® ERROR: Tailscale auth key is not set. Please set it in the first cell.\")\n",
        "else:\n",
        "    # Stop any running instances of tailscaled\n",
        "    print(\"üîÑ Ensuring previous Tailscale daemon is stopped...\")\n",
        "    subprocess.run(\"pkill -f tailscaled\", shell=True)\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Start the Tailscale daemon in the background using nohup and userspace-networking mode\n",
        "    print(\"üîÑ Starting Tailscale daemon in userspace mode...\")\n",
        "    daemon_command = \"nohup tailscaled --tun=userspace-networking --socket=/tmp/tailscaled.sock > /tmp/tailscaled.log 2>&1 &\"\n",
        "    subprocess.run(daemon_command, shell=True)\n",
        "    time.sleep(3) # Give the daemon a few seconds to initialize\n",
        "\n",
        "    # Check if the daemon is running\n",
        "    try:\n",
        "        subprocess.run([\"pgrep\", \"tailscaled\"], check=True, capture_output=True)\n",
        "        print(\"‚úÖ Tailscale daemon is running.\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"‚ùå CRITICAL: Tailscaled daemon failed to start.\")\n",
        "        print(\"   Check logs: !cat /tmp/tailscaled.log\")\n",
        "        OLLAMA_PRIVATE_URL = None # Prevent subsequent steps from running\n",
        "    else:\n",
        "        print(\"üöÄ Authenticating and connecting to Tailscale network...\")\n",
        "        # Point the 'up' command to the correct socket\n",
        "        tailscale_command = f'tailscale --socket=/tmp/tailscaled.sock up --authkey=\"{TAILSCALE_AUTH_KEY}\" --hostname=\"colab-ollama-runner\" --accept-routes'\n",
        "\n",
        "        try:\n",
        "            # Try to connect, and capture any errors\n",
        "            result = subprocess.run(tailscale_command, shell=True, check=True, capture_output=True, text=True)\n",
        "            print(\"‚úÖ Tailscale connected successfully in userspace mode.\")\n",
        "\n",
        "            # --- IP Address Retrieval by reading the log file (most robust method) ---\n",
        "            tailscale_ip = None\n",
        "            for i in range(10): # Retry up to 10 times\n",
        "                time.sleep(2) # Wait for logs to flush\n",
        "                print(f\"   Attempting to fetch IP from logs (attempt {i+1}/10)...\")\n",
        "                try:\n",
        "                    with open('/tmp/tailscaled.log', 'r') as log_file:\n",
        "                        logs = log_file.read()\n",
        "                        # Search for the line where the peerapi starts serving\n",
        "                        match = re.search(r\"peerapi: serving on http://(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}):\", logs)\n",
        "                        if match:\n",
        "                            tailscale_ip = match.group(1)\n",
        "                            break # Success! Exit the loop.\n",
        "                except FileNotFoundError:\n",
        "                    print(\"      ...log file not yet created, waiting.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"      ...error reading log file: {e}\")\n",
        "\n",
        "\n",
        "            if tailscale_ip:\n",
        "                print(f\"üîë Your private Ollama IP is: {tailscale_ip}\")\n",
        "                OLLAMA_PRIVATE_URL = f\"http://{tailscale_ip}:11434\"\n",
        "            else:\n",
        "                print(\"‚ùå Could not retrieve Tailscale IP after multiple attempts. Please check your Tailscale admin console and the log file '/tmp/tailscaled.log'.\")\n",
        "                OLLAMA_PRIVATE_URL = None\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            # If the command fails, print the specific error message from Tailscale\n",
        "            print(\"‚ùå Tailscale 'up' command failed. Error:\")\n",
        "            print(e.stderr)\n",
        "            print(\"\\nTroubleshooting Tips:\")\n",
        "            print(\"1. Have you already used this auth key? Generate a new 'ephemeral' and 'reusable' key.\")\n",
        "            print(\"2. Check for typos in your auth key.\")\n",
        "            OLLAMA_PRIVATE_URL = None\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Ensuring previous Tailscale daemon is stopped...\n",
            "üîÑ Starting Tailscale daemon in userspace mode...\n",
            "‚úÖ Tailscale daemon is running.\n",
            "üöÄ Authenticating and connecting to Tailscale network...\n",
            "‚úÖ Tailscale connected successfully in userspace mode.\n",
            "   Attempting to fetch IP from logs (attempt 1/10)...\n",
            "üîë Your private Ollama IP is: 100.82.13.29\n"
          ]
        }
      ],
      "id": "tailscale_connect_cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKJZFVkQQ4Fy"
      },
      "source": [
        "### Step 4: Start Ollama and Load Local Models"
      ],
      "id": "uKJZFVkQQ4Fy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyUe_1UUbAGM",
        "outputId": "95b1faf6-eddf-4ac0-ee0b-520f45179543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"üîÑ Preparing to start Ollama server...\")\n",
        "\n",
        "# Kill any previous Ollama processes to ensure a clean start\n",
        "try:\n",
        "    subprocess.run([\"pkill\", \"-f\", \"ollama\"], check=False)\n",
        "    print(\"Ensured previous Ollama processes are stopped.\")\n",
        "    time.sleep(2)\n",
        "except Exception as e:\n",
        "    print(f\"Could not pkill ollama, continuing: {e}\")\n",
        "\n",
        "# Set up environment for the Ollama subprocess\n",
        "ollama_env = os.environ.copy()\n",
        "ollama_env['OLLAMA_HOST'] = \"0.0.0.0\"  # Listen on all interfaces\n",
        "ollama_env['OLLAMA_ORIGINS'] = \"*\"      # Allow all origins (safe within a private Tailscale network)\n",
        "\n",
        "print(f\"Starting Ollama server, listening on {ollama_env['OLLAMA_HOST']}...\")\n",
        "\n",
        "# Start Ollama in the background\n",
        "ollama_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    env=ollama_env,\n",
        "    preexec_fn=os.setsid  # Detach from this notebook's session\n",
        ")\n",
        "\n",
        "print(f\"Ollama server process started with PID: {ollama_process.pid}\")\n",
        "print(\"Waiting for server to initialize...\")\n",
        "time.sleep(15)\n",
        "\n",
        "# --- Create Models from Local Filesystem ---\n",
        "MODELS_TO_CREATE = {\n",
        "    \"nomic-embed-text-local\": \"nomic-embed-text.mod\",\n",
        "    \"qwen3-8b-local\": \"qwen3_8b.mod\",\n",
        "    \"qwen3-30b-local\": \"qwen3_30b-a3b.mod\"\n",
        "}\n",
        "print(\"\\nüîÑ Creating models from local Modelfiles...\")\n",
        "for model_name, modelfile_name in MODELS_TO_CREATE.items():\n",
        "    modelfile_path = os.path.join(MODEL_PATH, modelfile_name)\n",
        "    print(f\"Attempting to create '{model_name}' from '{modelfile_path}'...\")\n",
        "\n",
        "    if not os.path.exists(modelfile_path):\n",
        "        print(f\"   ‚ùå Error: Modelfile not found at {modelfile_path}. Skipping.\")\n",
        "        continue\n",
        "    try:\n",
        "        create_command = [\"ollama\", \"create\", model_name, \"-f\", modelfile_path]\n",
        "        result = subprocess.run(\n",
        "            create_command,\n",
        "            capture_output=True, text=True, check=True, timeout=900  # 15-minute timeout for large models\n",
        "        )\n",
        "        print(f\"   ‚úÖ Successfully created model '{model_name}'.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"   ‚ùå Failed to create model '{model_name}':\\n      {e.stderr.strip()}\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(f\"   ‚ùå Error: Command timed out while creating '{model_name}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå An unexpected error occurred: {e}\")\n",
        "\n",
        "# --- Final Verification --- #\n",
        "print(\"\\nüìä Verifying available models with 'ollama list'...\")\n",
        "try:\n",
        "    result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True, check=True, timeout=20)\n",
        "    print(\"‚úÖ Ollama is running with the following models:\")\n",
        "    print(result.stdout)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to list Ollama models: {e}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Preparing to start Ollama server...\n",
            "Ensured previous Ollama processes are stopped.\n",
            "Starting Ollama server, listening on 0.0.0.0...\n",
            "Ollama server process started with PID: 6844\n",
            "Waiting for server to initialize...\n",
            "\n",
            "üîÑ Creating models from local Modelfiles...\n",
            "Attempting to create 'nomic-embed-text-local' from '/content/drive/MyDrive/ollamaModels/nomic-embed-text.mod'...\n",
            "   ‚úÖ Successfully created model 'nomic-embed-text-local'.\n",
            "Attempting to create 'qwen3-8b-local' from '/content/drive/MyDrive/ollamaModels/qwen3_8b.mod'...\n",
            "   ‚úÖ Successfully created model 'qwen3-8b-local'.\n",
            "Attempting to create 'qwen3-30b-local' from '/content/drive/MyDrive/ollamaModels/qwen3_30b-a3b.mod'...\n",
            "   ‚úÖ Successfully created model 'qwen3-30b-local'.\n",
            "\n",
            "üìä Verifying available models with 'ollama list'...\n",
            "‚úÖ Ollama is running with the following models:\n",
            "NAME                             ID              SIZE      MODIFIED               \n",
            "qwen3-30b-local:latest           8198b937d84e    18 GB     Less than a second ago    \n",
            "qwen3-8b-local:latest            c6af875d250d    5.0 GB    7 minutes ago             \n",
            "nomic-embed-text-local:latest    294e6a43cc6e    99 MB     10 minutes ago            \n",
            "\n"
          ]
        }
      ],
      "id": "TyUe_1UUbAGM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iVq1nShQ4Fz"
      },
      "source": [
        "### Step 5: Generate Zed Configuration\n",
        "\n",
        "This cell generates the `settings.json` content for Zed, using the private Tailscale IP for the connection."
      ],
      "id": "5iVq1nShQ4Fz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generate_zed_config_cell",
        "outputId": "99a57928-1c0a-4768-804a-0fe889d91f1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "\n",
        "if 'OLLAMA_PRIVATE_URL' in locals() and OLLAMA_PRIVATE_URL:\n",
        "    print(f\"Generating Zed configuration using private URL: {OLLAMA_PRIVATE_URL}\")\n",
        "\n",
        "    zed_settings = {\n",
        "        \"features\": {\n",
        "            \"edit_prediction_provider\": \"zed\"\n",
        "        },\n",
        "        \"language_models\": {\n",
        "            \"ollama\": {\n",
        "                \"api_url\": OLLAMA_PRIVATE_URL\n",
        "            }\n",
        "        },\n",
        "        \"agent\": {\n",
        "            \"version\": \"2\",\n",
        "            \"default_model\": {\n",
        "                \"provider\": \"ollama\",\n",
        "                \"model\": \"qwen3-8b-local\"\n",
        "            },\n",
        "            \"inline_assistant_model\": {\n",
        "                \"provider\": \"ollama\",\n",
        "                \"model\": \"qwen3-8b-local\"\n",
        "            },\n",
        "            \"thread_summary_model\": {\n",
        "                \"provider\": \"ollama\",\n",
        "                \"model\": \"qwen3-8b-local\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Display the configuration\n",
        "    print(\"\\nüìã Copy the following JSON into your Zed 'settings.json' file:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(json.dumps(zed_settings, indent=2))\n",
        "    print(\"-\" * 60)\n",
        "else:\n",
        "    print(\"‚ùå Cannot generate Zed config: Tailscale IP was not retrieved successfully.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Zed configuration using private URL: http://100.82.13.29:11434\n",
            "\n",
            "üìã Copy the following JSON into your Zed 'settings.json' file:\n",
            "------------------------------------------------------------\n",
            "{\n",
            "  \"features\": {\n",
            "    \"edit_prediction_provider\": \"zed\"\n",
            "  },\n",
            "  \"language_models\": {\n",
            "    \"ollama\": {\n",
            "      \"api_url\": \"http://100.82.13.29:11434\"\n",
            "    }\n",
            "  },\n",
            "  \"agent\": {\n",
            "    \"version\": \"2\",\n",
            "    \"default_model\": {\n",
            "      \"provider\": \"ollama\",\n",
            "      \"model\": \"qwen3-8b-local\"\n",
            "    },\n",
            "    \"inline_assistant_model\": {\n",
            "      \"provider\": \"ollama\",\n",
            "      \"model\": \"qwen3-8b-local\"\n",
            "    },\n",
            "    \"thread_summary_model\": {\n",
            "      \"provider\": \"ollama\",\n",
            "      \"model\": \"qwen3-8b-local\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "id": "generate_zed_config_cell"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzDUGJikQ4Fz"
      },
      "source": [
        "### Step 6: Final Instructions & Keep-Alive\n",
        "\n",
        "Your environment is now running. This final cell will keep the Colab instance active."
      ],
      "id": "HzDUGJikQ4Fz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PN4fawcdAF4",
        "outputId": "7604ec30-ac5f-4642-a763-5c536317b7fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import time\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üéâ SETUP COMPLETE! Your secure AI development environment is live.\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìã NEXT STEPS ON YOUR LOCAL COMPUTER:\")\n",
        "print(\"1. **Install Tailscale**: Download and install it from https://tailscale.com/download\")\n",
        "print(\"2. **Log In to Tailscale**: Use the same account you used to generate the auth key.\")\n",
        "print(\"3.  **Check Connection**: Run `tailscale status` in your local terminal. You should see the `colab-ollama-runner` machine listed.\")\n",
        "\n",
        "if 'tailscale_ip' in locals() and tailscale_ip:\n",
        "    print(f\"\\nYour private Ollama server is now accessible *only* to you at: http://{tailscale_ip}:11434\")\n",
        "else:\n",
        "    print(\"\\nCould not determine private Ollama IP. Please check the Tailscale setup cell.\")\n",
        "\n",
        "print(\"\\nThis cell will now run indefinitely to keep the Colab instance and Tailscale connection active.\")\n",
        "print(\"Press the 'Stop' button on this cell when you are finished to shut down the environment.\")\n",
        "\n",
        "# Keep-alive loop\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(300) # Sleep for 5 minutes\n",
        "        print(f\"[{time.strftime('%H:%M:%S')}] Keep-alive check. All services running.\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüõë Shutdown requested. All processes will terminate.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üéâ SETUP COMPLETE! Your secure AI development environment is live.\n",
            "================================================================================\n",
            "\n",
            "üìã NEXT STEPS ON YOUR LOCAL COMPUTER:\n",
            "1. **Install Tailscale**: Download and install it from https://tailscale.com/download\n",
            "2. **Log In to Tailscale**: Use the same account you used to generate the auth key.\n",
            "3.  **Check Connection**: Run `tailscale status` in your local terminal. You should see the `colab-ollama-runner` machine listed.\n",
            "\n",
            "Your private Ollama server is now accessible *only* to you at: http://100.82.13.29:11434\n",
            "\n",
            "This cell will now run indefinitely to keep the Colab instance and Tailscale connection active.\n",
            "Press the 'Stop' button on this cell when you are finished to shut down the environment.\n",
            "[22:44:59] Keep-alive check. All services running.\n",
            "[22:49:59] Keep-alive check. All services running.\n",
            "[22:54:59] Keep-alive check. All services running.\n",
            "[22:59:59] Keep-alive check. All services running.\n",
            "[23:04:59] Keep-alive check. All services running.\n",
            "[23:09:59] Keep-alive check. All services running.\n",
            "[23:14:59] Keep-alive check. All services running.\n",
            "[23:19:59] Keep-alive check. All services running.\n",
            "[23:24:59] Keep-alive check. All services running.\n",
            "[23:29:59] Keep-alive check. All services running.\n",
            "[23:34:59] Keep-alive check. All services running.\n",
            "[23:39:59] Keep-alive check. All services running.\n",
            "[23:44:59] Keep-alive check. All services running.\n",
            "[23:49:59] Keep-alive check. All services running.\n",
            "[23:54:59] Keep-alive check. All services running.\n",
            "[23:59:59] Keep-alive check. All services running.\n",
            "[00:04:59] Keep-alive check. All services running.\n",
            "[00:09:59] Keep-alive check. All services running.\n",
            "[00:14:59] Keep-alive check. All services running.\n",
            "[00:19:59] Keep-alive check. All services running.\n"
          ]
        }
      ],
      "id": "-PN4fawcdAF4"
    }
  ]
}